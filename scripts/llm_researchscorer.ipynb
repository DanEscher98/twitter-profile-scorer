{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "hTtOHOF4cv9O"
      },
      "outputs": [],
      "source": [
        "!pip install -q torch torchvision torchaudio\n",
        "!pip install -q xformers peft accelerate bitsandbytes trl\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "\n",
        "# For GGUF conversion (essential for Ollama deployment)\n",
        "!pip install -q llama-cpp-python gguf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "# Choose your model: Mistral 7B is recommended\n",
        "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "# Load the model with QLoRA configuration\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=model_name,\n",
        "    max_seq_length=2048,\n",
        "    dtype=torch.float16,\n",
        "    load_in_4bit=True,  # Activates QLoRA\n",
        ")\n",
        "\n",
        "# Define LoRA parameters\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state=3407,\n",
        "    max_seq_length=2048,\n",
        ")"
      ],
      "metadata": {
        "id": "AZHNRCfhmTkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load your JSONL dataset\n",
        "dataset_path = \"/content/research_dataset.jsonl\"\n",
        "dataset = load_dataset(\"json\", data_files=dataset_path, split=\"train\")\n",
        "\n",
        "print(f\"Loaded {len(dataset)} training examples\")\n",
        "\n",
        "# Format dataset for instruction tuning (NOT chat format)\n",
        "def formatting_function(examples):\n",
        "    \"\"\"\n",
        "    Format for instruction-following (not chat).\n",
        "    For a scoring model, we want instruction -> output directly.\n",
        "    \"\"\"\n",
        "    texts = []\n",
        "    for instruction, output in zip(examples[\"instruction\"], examples[\"output\"]):\n",
        "        # Mistral Instruct format for single-turn instruction following\n",
        "        text = f\"<s>[INST] {instruction} [/INST] {output}</s>\"\n",
        "        texts.append(text)\n",
        "    return {\"text\": texts}\n",
        "\n",
        "dataset = dataset.map(formatting_function, batched=True)\n",
        "\n",
        "# Show a sample\n",
        "print(\"\\nSample formatted example:\")\n",
        "print(dataset[0][\"text\"])"
      ],
      "metadata": {
        "id": "3dSJy2gjmcNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "import torch\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=2048,\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=5,\n",
        "        num_train_epochs=3,  # For 40 samples, consider 5-10 epochs\n",
        "        learning_rate=2e-4,\n",
        "        fp16=not torch.cuda.is_bf16_supported(),\n",
        "        bf16=torch.cuda.is_bf16_supported(),\n",
        "        logging_steps=1,\n",
        "        output_dir=\"mistral_profile_scorer_v1\",\n",
        "        optim=\"adamw_8bit\",\n",
        "        seed=42,\n",
        "        save_strategy=\"epoch\",  # Save after each epoch\n",
        "        save_total_limit=2,     # Keep only last 2 checkpoints\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "53AU13Hjme44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge LoRA weights with base model\n",
        "print(\"Merging LoRA adapter with base model...\")\n",
        "model.save_pretrained_merged(\n",
        "    \"final_merged_model\",\n",
        "    tokenizer,\n",
        "    save_method=\"merged_16bit\"  # Use 'merged_16bit' for full precision\n",
        ")\n",
        "print(\"✓ Merged model saved to: final_merged_model/\")"
      ],
      "metadata": {
        "id": "5lduayLBmj6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# --- Define Paths ---\n",
        "# Source directory where your merged model is saved\n",
        "HF_MODEL_DIR = '/content/final_merged_model'\n",
        "# Intermediate output file name (will be full size, e.g., 7GB)\n",
        "GGUF_OUTPUT_FP16 = '/content/profile_scorer.gguf'\n",
        "# Path to the conversion script\n",
        "# FIXED: Using the correct, modern script name from the llama.cpp repository\n",
        "CONVERT_SCRIPT = 'llama.cpp/convert_hf_to_gguf.py'\n",
        "\n",
        "print(f\"Starting conversion of {HF_MODEL_DIR} to unquantized GGUF...\")\n",
        "\n",
        "# Run the conversion script from the shell\n",
        "# We use the corrected script name here: convert_hf_to_gguf.py\n",
        "!python {CONVERT_SCRIPT} \\\n",
        "    --outfile {GGUF_OUTPUT_FP16} \\\n",
        "    --outtype f16 \\\n",
        "    {HF_MODEL_DIR}\n",
        "\n",
        "print(f\"\\n✓ Intermediate FP16 GGUF saved to: {GGUF_OUTPUT_FP16}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "7OpUBgK7zsmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enable inference mode\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# Test with a sample profile\n",
        "test_prompt = \"\"\"Score this Twitter profile. Given the following profile attributes, return a JSON object with: 'handle', 'score' (0.00-1.00 likelihood of being a real person who is an academic researcher), 'reason'. PROFILE ATTRIBUTES: handle: DrJaneSmith, displayName: Dr. Jane Smith, bio: Assistant Professor of Computer Science at MIT. PhD from Stanford. Research interests: ML, NLP., created_at: Mon Jan 15 10:30:00 +0000 2018, followerCount: 1523, location: Cambridge, MA\"\"\"\n",
        "\n",
        "inputs = tokenizer(\n",
        "    f\"<s>[INST] {test_prompt} [/INST]\",\n",
        "    return_tensors=\"pt\"\n",
        ").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=256,\n",
        "    temperature=0.1,  # Low temperature for consistent scoring\n",
        "    do_sample=False   # Deterministic output\n",
        ")\n",
        "\n",
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(\"\\nModel Response:\")\n",
        "print(response.split(\"[/INST]\")[-1].strip())"
      ],
      "metadata": {
        "id": "ClvvJPTamqz0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}